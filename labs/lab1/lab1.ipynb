{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 512 Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={mechanics:10}\n",
    "\n",
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: time complexity\n",
    "\n",
    "For each of the following functions, determine the time complexity as a function of the input $n$ using big-O notation and briefly justify your answer. If you get stuck, it's fair game to test things empirically and then try to understand what you observe. **Please state your assumptions if you donâ€™t know how long some operation in Python takes.** \n",
    "\n",
    "The first question is done for you, as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(n):\n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        print(i**2)\n",
    "        x = 9\n",
    "        y = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample answer**: The time complexity of `example` is  $O(n)$ because the function loops over $n$ elements and only performs constant-time operations inside the loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(a)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopy(n):\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            print('i =', i, '  j =', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopy(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(b)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangle(n):\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            print(\"+\", end='')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(c)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(n):\n",
    "    x = np.zeros(n)\n",
    "    x = x + 1000\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of x: ', len(foo(100000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 1(d)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(n):\n",
    "    x = np.zeros(1000)\n",
    "    x = x + n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('size of x: ', len(bar(100000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(e)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broken(n):\n",
    "    for i in range(n**2):\n",
    "        if i == n:\n",
    "            break  # \"break\" exits the innermost loop\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(f)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin(n):\n",
    "    i = n\n",
    "    while i > 1:\n",
    "        print('i = ', i)\n",
    "        i = i // 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `//` operator performs integer division, meaning the result is rounded *down* to the nearest integer. I believe this was mentioned in DSCI 511."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin(2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1(g)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin10(n):\n",
    "    i = n\n",
    "    while i > 1:\n",
    "        print('i = ', i)\n",
    "        i = i // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin10(2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1(h)\n",
    "rubric={reasoning:3}\n",
    "\n",
    "For this question, answer in terms of both $m$ and $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blahblah(n, m):\n",
    "    x = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            x = x + 1\n",
    "\n",
    "    for i in range(n):\n",
    "        x = x + 1\n",
    "\n",
    "    for i in range(m):\n",
    "        x = x + 1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blahblah(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1(i)\n",
    "rubric={reasoning:3}\n",
    "\n",
    "For this question, answer in terms of both $m$ and $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bllllergh(n, m):\n",
    "    x = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for k in range(m):\n",
    "                x = x + 1\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(m):\n",
    "                x = x + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllllergh(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 1(j)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cabin(n):\n",
    "    for i in range(n):\n",
    "        print('i = ', i)\n",
    "        for j in range(n//3):\n",
    "            print('j = ', j)\n",
    "            cabin(n)\n",
    "        print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cabin(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (optional) 1(k)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oh_no(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i = i - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: space complexity\n",
    "\n",
    "For each of the following functions, determine the space complexity as a function of the input $n$ using big-O notation and briefly justify your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(a)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(n):\n",
    "    x = np.random.rand(n)\n",
    "    y = np.random.rand(n)\n",
    "    total = 0\n",
    "    for x_i in x:\n",
    "        for y_i in y:\n",
    "            total += x_i*y_i\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(n):\n",
    "    x = np.zeros(1000)\n",
    "    x = x + n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c)\n",
    "rubric={reasoning:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FUNCTION(n):\n",
    "    x = set()\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x.add(j)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Timing for searching, sorting, hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Is searching in Python faster when the element you're looking for is that the start of the array? Design and run an \"experiment\" comparing the runtime of Python's `in` operator for the caes when the element being sought is at the start vs. the end of an array. Does it seem to make a difference? Briefly discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Is sorting in Python faster when the array you're sorting is already sorted? Design and run an \"experiment\" comparing the runtime of numpy's `.sort()` for the caes when the the array is vs. isn't already sorted. Does it seem to make a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={reasoning:5}\n",
    "\n",
    "We saw in class that hash tables like Python's `dict` grow when they get too full. Make a plot of the _size_ of a dictionary using `sys.getsizeof` vs. the number of elements. At what sizes does the dictionary seem to grow? Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 3(d)\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Now do the same experiment but with a `list` instead of a `dict`. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Markov Model of language\n",
    "\n",
    "_Meta-comment 1_: this is more of a programming exercise. There was some talk of having it in DSCI 511, but we ended up putting it here. However, it's definitely good practice in _using_ Python data structures like dictionaries. There are some optional questions about time/space complexity at the end. Overall, this is not a perfect thematic fit with DSCI 512, but it's very good practice (and hopefully fun!).\n",
    "\n",
    "_Meta-comment 2_: MDS-CL students: you will be seeing much more of this type of thing later on in the program. MDS-V students: we will do a bit more natural language processing in DSCI 575, but not a ton. All students: as an anti-FOMO measure, you will get access to course materials from MDS-V and MDS-CL after you complete the program.\n",
    "\n",
    "Moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to synthesize English text by \"learning\" from some input text, also known as a _corpus_. As an example, let's say the input text is the following, taken from the MDS website:\n",
    "\n",
    "> Data is everywhere. Continuously generated and collected across every domain, it is a vast and largely untapped resource of information with the potential to reveal insights about every aspect of our lives and the world we live in. However, the ability to uncover these insights is a highly specialized skill possessed by far too few. \n",
    "\n",
    "Our algorithm involves a parameter, which we'll call $n$. Let me first explain the approach when $n=1$. We will start with an initial character, say \"y\". There are 8 occurences of \"y\" in the input text above (you can count them!). What character typically comes after \"y\"? It turns out (according to the input text above) the next letter is \"w\" the first time and \" \" (space character) all the other 7 times. So we estimate the conditional probability distribution of the next character, given that the current character is \"y\", to be \"w\" with probability 1/8 and \" \" with probability 7/8 (and probability zero for all other characters). To generate the next character, we generate a sample from this simple distribution. Say we pick \" \", so we add a \" \" to our output text and it is now \"y \". Now \" \" is our current character. To generate the next character, we'd need to probability distribution of what comes after \" \" so that we could sample from it. We'd repeat this until the output text reaches a pre-specified length.\n",
    "\n",
    "What about larger $n$? For $n=3$, we pick the next character by looking at the _preceding 3 characters_. We use the name [_n-gram_](https://en.wikipedia.org/wiki/N-gram) for a sequence of $n$ characters. Our method should work for any $n>0$. For example, take our initial text to be the 3 characters \"is \". There are 3 occurrences of this $n$-gram in the text. In this case, the next letter is \"e\" once and \"a\" twice, so we estimate the conditional distribution to be 1/3 \"e\" and 2/3 \"a\" after \"is \". So we pick randomly from this distribution, and say we pick \"e\". Then our output text is now \"is e\" but our current $n$-gram is just \"s e\" because we're only using $n=3$. So to pick the next character after this, we'd look at what happens after occurrences of \"s e\". And so on.\n",
    "\n",
    "In order to implement this idea efficiently, you will pre-compute the conditional probabilitity distribution for every possible $n$ gram. To do that we need to count, for every possibly $n$-gram, the freqeuncies of the possible next characters, and then normalize them into probability distributions.\n",
    "\n",
    "*Attribution*: this exercise adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is, in my opinion, one of the best scientific papers ever written (in terms of both impact and readability).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grimms' Fairy Tales by Jacob and Wilhelm Grimm\n",
    "data_url = 'http://www.gutenberg.org/files/2591/2591-0.txt'\n",
    "corpus = urllib.request.urlopen(data_url).read().decode(\"utf-8\")\n",
    "\n",
    "# remove the first chunk of characters, which contains some header stuff\n",
    "corpus = corpus[2820:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:200])  # print out the first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(a): implementation\n",
    "rubric={accuracy:10,quality:10}\n",
    "\n",
    "You will implement the above algorithm in a class called `MarkovModel`. Your class will have the following methods:\n",
    "\n",
    "- `__init__`, which is already implemented from you.\n",
    "- `fit`, which calculates and stores the _frequencies_ of all possible next characters given an $n$-gram. These frequencies should be stored in a dict of dicts, where the keys of the outer dict are the $n$-grams and the keys of the inner dict are the possible next characters, and the values of the inner dict are the frequencies (counts). Then, at the end of `fit`, normalize these frequencies into empirical probabilities and store them in `self.probabilities`. Note: before starting the calculations, append the first $n$ characters of your corpus to the end of the corpus, making it \"circular\"; this will avoid a situation where you your `generate` function might get stuck.\n",
    "- `generate`, which creates a random text of a specified length by generating one character at a time from the appropriate (discrete) probability distribution. To perform the random sampling, use the `p` argument of `np.random.choice`. You can start the output text with the first $n$ characters of the input text.\n",
    "\n",
    "Note: you may find some of the fancy dictionaries in the [`collections`](https://docs.python.org/3.7/library/collections.html) package useful, namely `defaultdict` and/or `Counter`. However, you can also just use `dict`; either way is fine.\n",
    "\n",
    "Hint: if you find yourself searching for all occurrences of an $n$-gram in the text, you are approaching this incorrectly - in that case, ask us for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    \"\"\"A Markov model of languages based on character frequencies in text.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.probabilities = None \n",
    "        self.starting_chars = None\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov model and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            a corpus of text \n",
    "        \"\"\"\n",
    "        \n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[:self.n]\n",
    "        \n",
    "        # make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[:self.n]\n",
    "\n",
    "        # Step 1: Compute frequencies\n",
    "        # FILL IN THE REST OF THE CODE HERE\n",
    "        \n",
    "        # Step 2: Normalize the frequencies into probabilities\n",
    "        # FILL IN THE REST OF THE CODE HERE\n",
    "        \n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Generate a sequence of length seq_len, Markov model learned in `fit`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        s = self.starting_chars\n",
    "        while len(s) < seq_len:\n",
    "            current_ngram = s[-self.n:]\n",
    "            \n",
    "            # FILL IN THE REST OF THE CODE HERE\n",
    "            \n",
    "        return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tests that should pass if `fit` is implemented correctly (it will not work yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MarkovModel(n=2)\n",
    "test_corpus = \"2 + 2 = 4; 2 + 3 = 5; 3 + 3 = 9; 3 + 2 = 5;\"\n",
    "mm.fit(test_corpus)\n",
    "assert mm.starting_chars == '2 '\n",
    "assert mm.probabilities['2 ']['symbols'][0] == '+'\n",
    "assert mm.probabilities['2 ']['probs'][0] == 1/2\n",
    "assert mm.probabilities[' 3']['symbols'][0] == ' '\n",
    "assert mm.probabilities[' 3']['probs'][0] == 1\n",
    "assert mm.probabilities[';2']['symbols'][0] == ' '\n",
    "assert mm.probabilities[';2']['probs'][0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm.generate(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we run it on our fairy tales corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MarkovModel(n=5)\n",
    "mm.fit(corpus)\n",
    "mm.generate(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mm = MarkovModel(n=2)\n",
    "test_corpus = \"2 + 2 = 4; 2 + 3 = 5; 3 + 3 = 9; 3 + 2 = 5;\"\n",
    "mm.fit(test_corpus)#### 4(b): fun with language models\n",
    "rubric={reasoning:5}\n",
    "\n",
    "1. Explain what happens as you increase $n$ from 1 to larger and larger values. At what point does it start to look like English? At what point is your model just memorizing the input corpus?\n",
    "2. Generate some random sequences using the data set of your choice. Submit your favourite randomly generated sequence as well as the link to the data you used to generate it. If you are out of ideas, you may find some text files of popular books [here](http://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 4(c): time complexity of `fit`\n",
    "rubric={reasoning:1}\n",
    "\n",
    "For the above implementation, what is the (worst case) time complexity of running `fit` in terms of:\n",
    "\n",
    "- $n$, the length of each $n$-gram\n",
    "- the length of the corpus, which we'll call $N$\n",
    "- the length of the sequence to generate, `seq_len`, which we'll call $T$\n",
    "\n",
    "You can assume `np.random.choice` takes $O(1)$ time. You can also assume $n \\ll N$ and $n \\ll T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Exercise 4d: time complexity of `generate`\n",
    "rubric={reasoning:1}\n",
    "\n",
    "For the above implementation, what is the (worst case) time complexity of running `generate` in terms of $n$, $N$, and $T$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Exercise 4(e): total time complexity\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What is the total time complexity of running `fit` once and then `generate` once, in terms of $n$, $N$, and $T$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 4(f): space complexity\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What are the space complexities of `fit` and `generate`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsci512env]",
   "language": "python",
   "name": "conda-env-dsci512env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
